{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7a2f74-6b81-49c3-ad21-30048746e763",
   "metadata": {},
   "source": [
    "## DATA EXTRACTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce371e-abe3-4cf2-ae54-f1583d9162f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc6e5c-62c6-43e7-ab0e-ead337c25c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = pd.read_excel('input.xlsx') #add ur input file here it should contain a url_id and urls of website u want to crawl to\n",
    "input_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80c32a-1fb5-4644-ba85-e669774a24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd304551-5a3f-4911-8db2-b96b25236d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97f1f5-7e96-4d28-95c4-1f85a2186f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in input_file.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "    # print(index)\n",
    "    \n",
    "    article =  \" \" #storing the extracted data\n",
    "    \n",
    "    # code to fetch data\n",
    "    try:\n",
    "        page=requests.get(url)\n",
    "        # print(page)\n",
    "    except:\n",
    "        print(\"Cant get response\")\n",
    "    \n",
    "    try:\n",
    "        soup=BeautifulSoup(page.content,'html.parser')\n",
    "        title = soup.find('h1').get_text()\n",
    "        for p in soup.find_all('p'):\n",
    "            article += p.get_text()\n",
    "        # print(article)\n",
    "        file_name = 'extracted/' + str(url_id) + '.txt'\n",
    "        with open(file_name, 'w') as file:\n",
    "            file.write(title + '\\n' + article)\n",
    "        print(f\"Success {url_id}\")\n",
    "    except:\n",
    "        print(f\"not found {url_id}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af1311-efef-4b95-aa0c-0336bf8b51bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file.drop([35,48,18,24,25,78,87], axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8e89f-64a2-4ee2-b116-77964cf68028",
   "metadata": {},
   "source": [
    "## Creating a dictionary of Positive and Negative words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdd7f7-deba-4aa8-a21c-a58f82c1727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dir = 'extracted/'\n",
    "stop_dir = 'StopWords/'\n",
    "master_dir = 'MasterDictionary/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60410af-85ef-4d9e-aa3a-3b45e701fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #setting up the stopwords\n",
    "stop_words = set()\n",
    "for file in os.listdir(stop_dir):\n",
    "      with open(os.path.join(stop_dir,file),'r') as f:\n",
    "            stop_words.update(set(f.read().splitlines()))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15f0ea-a6b5-479c-85d6-58f4e86a9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up positive words\n",
    "pos_words = set()\n",
    "neg_words = set()\n",
    "\n",
    "for files in os.listdir(master_dir):\n",
    "  if files =='positive-words.txt':\n",
    "    with open(os.path.join(master_dir,files),'r') as f:\n",
    "      pos_words.update(f.read().splitlines())\n",
    "  else:\n",
    "    with open(os.path.join(master_dir,files),'r') as f:\n",
    "      neg_words.update(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89823c1-faed-43cb-be16-282ffc3dbb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684a749-3194-4ed3-a357-77a0f7fd7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a55cf5-4305-44d2-8e7c-901a30ae4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing text file and creating a doc for it \n",
    "\n",
    "docs = []\n",
    "for text_file in os.listdir(text_dir):\n",
    "  with open(os.path.join(text_dir,text_file),'r') as f:\n",
    "    text = f.read()\n",
    "    words = word_tokenize(text)\n",
    "    broken = [word for word in words if word.lower() not in stop_words]\n",
    "    docs.append(broken)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c77c9-d988-41de-a698-88a9119ab54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12bc09-d42f-408c-a5bf-448b16459402",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[3] # checking if it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2987c5f6-6ea3-4fa9-9f3b-3897dd903245",
   "metadata": {},
   "source": [
    "## Extracting Derived variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8209a4-f939-42c2-abbc-1f5d4ba8c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = []\n",
    "negative_words =[]\n",
    "positive_score = []\n",
    "negative_score = []\n",
    "polarity_score = []\n",
    "subjectivity_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408dc4c4-dad5-4a18-bac5-343e28b1804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(docs)):\n",
    "    for word in docs[i]:\n",
    "        if word.lower() in pos_words:\n",
    "            positive_words.append(word)\n",
    "            # print(positive_words)\n",
    "    for word in docs[i]:\n",
    "        if word.lower() in neg_words:\n",
    "            negative_words.append(word)\n",
    "\n",
    "    positive_score.append(len(positive_words[i]))\n",
    "    negative_score.append(len(negative_words[i]))\n",
    "    polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n",
    "    subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f2fac-4286-401d-8525-ebd98b1df35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(positive_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab32a949-9643-4987-ae34-3bbd03d7b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(negative_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb58613-4a23-4a31-a8c0-a86db1c4356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(polarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbad03c-e53f-4997-862c-be403ab0ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subjectivity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046fe96f-b323-4fbc-a7ad-11402c189128",
   "metadata": {},
   "source": [
    "## Analysis of Readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7b2af-8253-4c54-b913-d29f8933b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_sentence_length = []\n",
    "percentage_of_complex_words = []\n",
    "fog_index = []\n",
    "number_of_complex_words = []\n",
    "syllable_count_per_word =[]\n",
    "average_number_of_words_per_sentence = []\n",
    "stp_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d74789-aaa7-4b26-95a6-5ef49aedba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_sentences(file):\n",
    "    with open(os.path.join(text_dir, file),'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "        total_sentences = []\n",
    "        total_words = []\n",
    "        sentences_broken = sent_tokenize(text)\n",
    "        for x in sentences_broken:\n",
    "            total_sentences.append(sentences_broken)\n",
    "            for word in x.split():\n",
    "                total_words.append(word)\n",
    "\n",
    "        total_sentences_length = len(total_sentences)\n",
    "        total_word_length = len(total_words)\n",
    "        \n",
    "        # print(toal_word_length)\n",
    "\n",
    "\n",
    "        \n",
    "        pattern = r'[^\\w\\s.]'\n",
    "        text = re.sub(pattern,' ',text)\n",
    "        sentences = re.split(r'[.!?]\\s*', text)\n",
    "        \n",
    "\n",
    "      \n",
    "        num_sentences = len(sentences)\n",
    "\n",
    "        word_in_sentences = []\n",
    "        for word in text.split():\n",
    "            if word.lower() not in stp_words:\n",
    "                  word_in_sentences.append(word)\n",
    "        num_of_words = len(word_in_sentences)\n",
    "\n",
    "        complex_word = []\n",
    "        for word in word_in_sentences:\n",
    "           \n",
    "            vowels = 'aeiou'\n",
    "            syllables = 0\n",
    "            for letter in word:\n",
    "                if letter.lower() in vowels:\n",
    "                    syllables = syllables + 1 \n",
    "            \n",
    "            if syllables>2:\n",
    "                complex_word.append(word)\n",
    "        \n",
    "\n",
    "        syllable_count = 0\n",
    "        syllable_count_word = []\n",
    "        \n",
    "        for word in word_in_sentences:\n",
    "            if word.endswith('es'):\n",
    "                    word = word[:-2]\n",
    "            elif word.endswith('ed'):\n",
    "                    word = word[:-2]\n",
    "            vowels = 'aeiou'\n",
    "            for letter in word:\n",
    "                if letter.lower() in vowels:\n",
    "                    syllable_count = syllable_count + 1\n",
    "            if syllable_count >= 1:\n",
    "                    syllable_count_word.append(word)\n",
    "           \n",
    "                \n",
    "                    \n",
    "\n",
    "                    \n",
    "        number_of_complex_words = len(complex_word)            \n",
    "            \n",
    "        avg_sen_len = num_of_words / num_sentences\n",
    "        if num_of_words == 0:\n",
    "            per_complex_word = 0\n",
    "        else:\n",
    "            per_complex_word = len(complex_word)/num_of_words\n",
    "            \n",
    "        fog = 0.4 * (avg_sen_len + per_complex_word)   \n",
    "\n",
    "        if syllable_count == 0:\n",
    "            syllable_count_per_word= 0\n",
    "        else:\n",
    "            syllable_count_per_word = syllable_count / len(syllable_count_word)\n",
    "\n",
    "\n",
    "        if total_word_length ==0:\n",
    "            average_number_of_words_per_sentence = 0\n",
    "        else:\n",
    "            average_number_of_words_per_sentence = total_word_length/ total_sentences_length\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        return avg_sen_len , per_complex_word , fog , syllable_count_per_word , average_number_of_words_per_sentence, number_of_complex_words\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d626ab5-962d-4afb-a843-5939fef00c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(text_dir):\n",
    "    x,y,z,k,l, m= finding_sentences(file)\n",
    "    average_sentence_length.append(x)\n",
    "    percentage_of_complex_words.append(y)\n",
    "    fog_index.append(z)\n",
    "    syllable_count_per_word.append(k)\n",
    "    average_number_of_words_per_sentence.append(l)\n",
    "    number_of_complex_words.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d5e34-f4fa-423e-8ac5-aae1354b4067",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(average_number_of_words_per_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4668721-6303-4041-a198-72a1eab2cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(percentage_of_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe5ded7-f578-43b6-8625-55f790f862a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(number_of_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e79047b-6f07-4994-9b92-54c808590a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fog_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005cefd-3a75-491f-abce-040135f73dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(syllable_count_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08d5ddc-8741-4c28-bfd8-afe8a32cc10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(average_number_of_words_per_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea40a98-4d36-4ff9-97ea-1554788d9142",
   "metadata": {},
   "source": [
    "## Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993ddd3-57b7-425f-a323-c27169264641",
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_pronoun = [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec387593-00fe-4edb-bf0c-0aade11ec878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_pronouns(file):\n",
    "  with open(os.path.join(text_dir,file), 'r') as f:\n",
    "    text = f.read()\n",
    "    pronoun_regex = r\"\\b(I|me|my|mine|you|your|yours|he|him|his|she|her|hers|it|its|they|them|their|theirs|we|us|our|ours)\\b\"\n",
    "    pronoun_regex = rf\"{pronoun_regex}(?!\\sUS)\"\n",
    "    matches = re.findall(pronoun_regex, text, flags=re.IGNORECASE)\n",
    "    count = len(matches)\n",
    "  return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5c59e-f494-4989-b5a1-94983c945ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(text_dir):\n",
    "  x = personal_pronouns(file)\n",
    "  personal_pronoun.append(x)\n",
    "len(personal_pronoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a272f01-ea9a-4f8c-a071-51291a1ee1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_len = []\n",
    "word_count = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d394c1-0e9e-46a2-a2e0-4378a1ba087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(file):\n",
    "     with open(os.path.join(text_dir,file), 'r') as f:\n",
    "        text = f.read()\n",
    "        pattern = r'[^\\w\\s.]'\n",
    "        text = re.sub(pattern,' ',text)\n",
    "        sentences = re.split(r'[.!?]\\s*', text)\n",
    "        num_sentences = len(sentences)\n",
    "\n",
    "        word_in_sentences = []\n",
    "        for word in text.split():\n",
    "            if word.lower() not in stp_words:\n",
    "                  word_in_sentences.append(word)\n",
    "        num_of_words = len(word_in_sentences)\n",
    "\n",
    "\n",
    "        length = 0\n",
    "        for word in word_in_sentences:\n",
    "            length = length + len(word)\n",
    "\n",
    "        if num_of_words == 0:\n",
    "            average_word_length = 0\n",
    "        else:\n",
    "            average_word_length = length / num_of_words\n",
    "        return num_of_words,average_word_length\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46218b8b-d6d6-43bf-8fee-6d6fd8b29532",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(text_dir):\n",
    "  x, y = avg_word(file)\n",
    "  word_count.append(x)\n",
    "  avg_word_len.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d9ab58-e03f-4ed9-88ea-fa3170302c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(avg_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327f2b5-35f2-4fba-8d59-2619e2a5b6fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04bb5ee-6f4e-43c5-9d7d-a5519bcb1cfb",
   "metadata": {},
   "source": [
    "## Output Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499e2b7-e3e9-43b1-8968-455206c711c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = pd.read_excel('output.xlsx') #this should contain ur output datastructure if not present u create ur own \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691472d-5611-4f8b-969e-8f71c1bc93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab0bae8-d865-4101-b62d-819248f34414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output_data.drop([35,48], axis = 0, inplace=True)\n",
    "\n",
    "variables_present = [positive_score,\n",
    "            negative_score,\n",
    "            polarity_score,\n",
    "            subjectivity_score,\n",
    "            average_sentence_length,\n",
    "            percentage_of_complex_words,\n",
    "            fog_index,\n",
    "            average_number_of_words_per_sentence,\n",
    "            number_of_complex_words,\n",
    "            word_count ,\n",
    "            syllable_count_per_word,\n",
    "            personal_pronoun,\n",
    "            avg_word_len]\n",
    "\n",
    "# variable = []\n",
    "\n",
    "# for x in output_data.columns:\n",
    "#     variable.append(x)\n",
    "\n",
    "# df = pd.DataFrame(columns=variable)\n",
    "\n",
    "# data = {'POSITIVE SCORE': positive_score , \n",
    "#         'NEGATIVE SCORE' : negative_score,\n",
    "#         'POLARITY SCORE' : polarity_score,\n",
    "#        'SUBJECTIVITY SCORE': subjectivity_score,\n",
    "#        'AVG SENTENCE LENGTH' : average_sentence_length,\n",
    "#        'PERCENTAGE OF COMPLEX WORDS': percentage_of_complex_words,\n",
    "#         'FOG INDEX' : fog_index,\n",
    "#         'AVG NUMBER OF WORDS PER SENTENCE' : average_number_of_words_per_sentence,\n",
    "#         'COMPLEX WORD COUNT' : number_of_complex_words,\n",
    "#         'WORD COUNT' : word_count,\n",
    "#         'SYLLABLE PER WORD' : syllable_count_per_word,\n",
    "#         'PERSONAL PRONOUNS' : personal_pronoun,\n",
    "#         'AVG WORD LENGTH' : avg_word_len\n",
    "#        }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "# df.drop(\n",
    "# df\n",
    "\n",
    "output_data\n",
    "\n",
    "for i, var in enumerate(variables_present):\n",
    "  output_data.iloc[:,i+2] = var\n",
    "\n",
    "output_data.to_csv('output_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a1446e-536d-4d3c-8651-595f68c9e093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
